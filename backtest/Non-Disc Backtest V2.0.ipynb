{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import *\n",
    "import pandas as pd\n",
    "from datetime import *\n",
    "import time\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filter= 'Treynor'\n",
    "metrics = [ 'PE_RATIO', 'PX_TO_BOOK_RATIO', 'TRAIL_12M_EPS', 'TOT_DEBT_TO_TOT_EQY', \\\n",
    "               'PX_TO_FREE_CASH_FLOW', 'RETURN_COM_EQY', 'RETURN_ON_ASSET' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anying in all caps are places where I think there can be some improvement or places where we need to figure things out still."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Brings in Bloomberg data\n",
    "totalDF = read_csv(\"Backtest VALUES 2020.csv\",index_col='DATE',parse_dates=True)\n",
    "#Sorts the data by date\n",
    "totalDF = totalDF.sort_index()\n",
    "#Gets rid of any blank frames\n",
    "totalDF.dropna(axis='index',how='any',inplace=True)\n",
    "#Makes a list of unique dates\n",
    "dates= totalDF['DATE'].unique()\n",
    "\n",
    "#filters out the over preforming and underpreforming companies in each quarter and creates a new CSV with only strong preformers\n",
    "for d in dates:\n",
    "    #grabs frames from the total data for that quarter\n",
    "    working = totalDF.loc(totalDF['DATE'] == d)\n",
    "    #finds 80th and 95th precentile values. I WOULD LOVE YOUR GUYS' THOUGHTS ON THE RANGES WE SHOULD  BE USING. WOULD BE INTERESTING TO CALCULATE BEST PRECETILE VALUES\n",
    "    #DO YOU GUYS KNOW ANY WAYS TO USE STATISTICS TO FIND SOILD RANGES?\n",
    "    #Again finds companies  with treynors between the 80th and 95th precentiles in the given quarter\n",
    "    eighty= float np.precentile(working.loc(working[Filter]).dropna(), 75)\n",
    "    ninety= float np.precentile(working.loc(working[Filter]).dropna(), 85)\n",
    "    #Creates a new data set with only data in the 80-95 precentile range\n",
    "    dt= working[(working[Filter] >= eighty) & (working[Filter] <= ninety)]\n",
    "    #Adds the data to a new data set\n",
    "    preformdata.append(dt)\n",
    "\n",
    "#could also just round every valuation to nearest int, then compare then find unique sets to preserve the entire data set\n",
    "\n",
    "#finds a spread proportional to range of each metric\n",
    "def spreadfinder(m):\n",
    "    return (preformdata[m].max()-preformdata[m].min())*.05 #*(preformdata[m].mean()- val)\n",
    "#this could be function of how close value is to mean\n",
    "#THOUGHTS ON OTHER WAYS THIS COULD BE IMPROVED\n",
    "#bell curve/ standard deviation.\n",
    "#\n",
    "\n",
    "PEspr=spreadfinder('PE_RATIO')\n",
    "PBspr=spreadfinder('PX_TO_BOOK_RATIO')\n",
    "TTMEPSspr=spreadfinder('TRAIL_12M_EPS')\n",
    "DtoEQspr=spreadfinder('TOT_DEBT_TO_TOT_EQY')\n",
    "PFCFspr=spreadfinder('PX_TO_FREE_CASH_FLOW')\n",
    "RonEQspr=spreadfinder('RETURN_COM_EQY')\n",
    "RonAspr=spreadfinder('RETURN_ON_ASSET')\n",
    "\n",
    "columns=dates.extend(['Averag Treynor', 'Frame Values']\n",
    "output=pd.DataFrame(columns= dates.extend(['Averag Treynor', 'Frame Values']))\n",
    "worklist=[]\n",
    "#goes through every set of valuations for companies in the preformance data set, which was the set of data that filtered out underpreforming companies\n",
    "for df in preformdata:\n",
    "    worklist.clear()\n",
    "    for d in dates\n",
    "        working = totalDF.loc(totalDF['DATE'] == d)\n",
    "        #finds average treynor/filter metirc of all of the companies\n",
    "        frmavg=working[(working['PE_RATIO'] <= PEspr+df['PE_RATIO']) & (working['PE_RATIO'] >= PEspr-df['PE_RATIO'])\\\n",
    "                & (working['PX_TO_BOOK_RATIO'] <= PBspr+df['PX_TO_BOOK_RATIO']) & (working['PX_TO_BOOK_RATIO'] >= PBspr-df['PX_TO_BOOK_RATIO'])\\ \n",
    "                & (working['TRAIL_12M_EPS'] <= TTMEPSspr+df['TRAIL_12M_EPS']) & (working['TRAIL_12M_EPS'] >= TTMspr-df['TRAIL_12M_EPS'])\\\n",
    "                & (working['TOT_DEBT_TO_TOT_EQY'] <= DtoEQspr+df['TOT_DEBT_TO_TOT_EQY']) & (working['TOT_DEBT_TO_TOT_EQY'] >= DtoEQspr-df['TOT_DEBT_TO_TOT_EQY'])\\\n",
    "                & (working['PX_TO_FREE_CASH_FLOW'] <= PFCFspr+df['PX_TO_FREE_CASH_FLOW'])& (working['PX_TO_FREE_CASH_FLOW'] >= PFCFspr-df['PX_TO_FREE_CASH_FLOW'])\\ \n",
    "                & (working['RETURN_COM_EQY'] <= RonEQspr+df['RETURN_COM_EQY']) & (working['RETURN_COM_EQY'] >= RonEQspr-df['RETURN_COM_EQY'])\\\n",
    "                & (working['RETURN_ON_ASSET'] <= RonAspr+df['RETURN_ON_ASSET']) & (working['RETURN_ON_ASSET'] >= RonAspr-df['RETURN_ON_ASSET'])& Filter].mean()\n",
    "        #this could be improved by weighting mean by \"distances\" from frame value\n",
    "        worklist.append(frmavg)\n",
    "    treyavg=sum(worklist)/len(worklist)    \n",
    "    worklist.append(treyavg,df)\n",
    "    output.append({columns:worklist}, ignore_index=True)\n",
    "        #use list and apend values onto it \n",
    "output.to_csv('output.csv', index=False) \n",
    "        #add to list of working values  HOW SHOULD WE DO THIS? I THINK WE WANT TO LIMIT THE AMOUNT OF WRITING THE CODE DOES.\n",
    "        #TO REDUCE RUN TIME. WE COULD HAVE IT ADD THE VALUES TO A LIST?\n",
    "        \n",
    "        \n",
    "    #average out values in list\n",
    "    \n",
    "    \n",
    "    #this could be improved by weighting preformace by how poor overall market was doing during that time\n",
    "        \n",
    "        \n",
    "#sort the output sheet from high to low, could be done in excel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOU GUYS CAN IGNORE THIS\n",
    "#compressed preformance filter\n",
    "for d in dates:\n",
    "    dt= totalDF.loc(totalDF['DATE'] == d)[(totalDF.loc(totalDF['DATE'] == d)[Filter] >= float np.precentile(totalDF.loc(totalDF['DATE'] == d).loc(totalDF.loc(totalDF['DATE'] == d)[Filter]).dropna(), 80))\\\n",
    "                & (totalDF.loc(totalDF['DATE'] == d)[Filter] <= float np.precentile(totalDF.loc(totalDF['DATE'] == d).loc(totalDF.loc(totalDF['DATE'] == d)[Filter]).dropna(), 95))]\n",
    "    preformdata.append(dt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
